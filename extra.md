


I'll help you plan the implementation to match `orka_or`'s behavior exactly. Let me first analyze the key components we need to modify:

Let me search for the core components in `orka_or` that handle these features:
Search files...
Let me also check how memory storage is handled:
Search files...
Let me also check how past loops are handled:
Search files...
Based on the analysis of `orka_or`'s implementation, here's the detailed plan to implement the same behavior in the new version:

1. **Template Rendering Fix**:
```python
# orka/orchestrator/prompt_rendering.py

def _enhance_payload_for_templates(self, payload: Dict[str, Any]) -> Dict[str, Any]:
    """Enhance payload for better template rendering."""
    enhanced_payload = payload.copy()

    # Expose key properties from input object at root level
    if isinstance(enhanced_payload.get("input"), dict):
        input_data = enhanced_payload["input"]
        for key, value in input_data.items():
            if key not in enhanced_payload:  # Don't override existing keys
                enhanced_payload[key] = value

    # Expose commonly used template variables at root level
    if "previous_outputs" in enhanced_payload:
        for key in ["loop_number", "past_loops_metadata"]:
            if key in enhanced_payload["input"]:
                enhanced_payload[key] = enhanced_payload["input"][key]

        # Expose agent outputs at root level for easy access
        for agent_id, agent_output in enhanced_payload["previous_outputs"].items():
            if isinstance(agent_output, dict):
                for key in ["response", "formatted_prompt", "internal_reasoning"]:
                    if key in agent_output:
                        enhanced_payload[f"{agent_id}_{key}"] = agent_output[key]

    return enhanced_payload
```

2. **Memory Storage Fix**:
```python
# orka/nodes/memory_writer_node.py

def _extract_memory_content(self, context: Dict[str, Any]) -> List[str]:
    """Extract all memory objects from validation guardian outputs."""
    memory_objects: List[str] = []
    
    previous_outputs = context.get("previous_outputs", {})
    
    # Check both validation guardians
    for guardian_name in ["true_validation_guardian", "false_validation_guardian"]:
        if guardian_name in previous_outputs:
            guardian_output = previous_outputs[guardian_name]
            if isinstance(guardian_output, dict) and "result" in guardian_output:
                result = guardian_output["result"]
                
                # Handle direct memory objects
                if isinstance(result, dict) and "memory_objects" in result:
                    for mem_obj in result["memory_objects"]:
                        if isinstance(mem_obj, str):
                            memory_objects.append(mem_obj)
                        elif isinstance(mem_obj, dict) and "content" in mem_obj:
                            memory_objects.append(mem_obj["content"])
                
                # Handle nested memory objects
                if isinstance(result, dict):
                    for key, value in result.items():
                        if isinstance(value, dict) and "memory_objects" in value:
                            for mem_obj in value["memory_objects"]:
                                if isinstance(mem_obj, str):
                                    memory_objects.append(mem_obj)
                                elif isinstance(mem_obj, dict) and "content" in mem_obj:
                                    memory_objects.append(mem_obj["content"])
    
    return memory_objects
```

3. **Past Loops Fix**:
```python
# orka/nodes/loop_node.py

async def _execute_internal_workflow(
    self,
    original_input: Any,
    previous_outputs: Dict[str, Any]
) -> Optional[Dict[str, Any]]:
    """Execute the internal workflow configuration."""
    from ..orchestrator import Orchestrator
    import os
    import tempfile
    
    # Get the original workflow configuration
    original_workflow = self.internal_workflow.copy()
    
    # Ensure we have the basic structure
    if "orchestrator" not in original_workflow:
        original_workflow["orchestrator"] = {}
    
    # Update the orchestrator configuration while preserving agents
    orchestrator_config = original_workflow["orchestrator"]
    orchestrator_config.update({
        "id": orchestrator_config.get("id", "internal-workflow"),
        "strategy": orchestrator_config.get("strategy", "sequential"),
        "memory": {
            "config": {
                "redis_url": os.getenv("REDIS_URL", "redis://localhost:6380/0"),
                "index_name": self.memory.index_name if self.memory else "orka_enhanced_memory",
                "embedding_model": self.memory.embedding_model if self.memory else "text-embedding-ada-002",
                "embedding_dim": self.memory.embedding_dim if self.memory else 1536,
                "default_short_term_hours": self.memory.default_short_term_hours if self.memory else 0.1,
                "default_long_term_hours": self.memory.default_long_term_hours if self.memory else 0.2,
                "check_interval_minutes": self.memory.check_interval_minutes if self.memory else 30,
            }
        }
    })
    
    # Calculate current loop number from past_loops length
    current_loop_number = len(previous_outputs.get("past_loops", [])) + 1
    
    # Prepare input with past_loops context
    internal_workflow_input = {
        "input": original_input,
        "previous_outputs": previous_outputs,
        "loop_number": current_loop_number,
        "past_loops_metadata": {
            "insights": self._extract_metadata_field("insights", previous_outputs.get("past_loops", [])),
            "improvements": self._extract_metadata_field("improvements", previous_outputs.get("past_loops", [])),
            "mistakes": self._extract_metadata_field("mistakes", previous_outputs.get("past_loops", [])),
        }
    }
    
    # Create a temporary YAML file for the internal workflow
    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".yml") as temp_file:
        yaml.dump(original_workflow, temp_file, default_flow_style=False)
        temp_file_path = temp_file.name
    
    try:
        # Create orchestrator for internal workflow, passing the existing memory logger
        orchestrator = Orchestrator(temp_file_path, memory_logger=self.memory)
        orchestrator_result = await orchestrator.run(internal_workflow_input)
        return orchestrator_result
    except Exception as e:
        logger.error(f"Failed to execute internal workflow: {e}")
        raise
    finally:
        # Clean up the temporary file
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
```

4. **PastLoopMetadata Type Update**:
```python
# orka/nodes/loop_node.py

class PastLoopMetadata(TypedDict, total=False):
    loop_number: int
    score: float
    timestamp: str
    insights: str
    improvements: str
    mistakes: str
    result: Dict[str, Any]
    input: str
    response: str
    formatted_prompt: str
    internal_reasoning: str
    _metrics: Dict[str, Any]
```

5. **Memory Logger Configuration**:
```python
# orka/memory/redisstack_logger.py

def __init__(
    self,
    redis_url: str = "redis://localhost:6379/0",
    index_name: str = "orka_enhanced_memory",
    embedder=None,
    memory_decay_config: Optional[Dict[str, Any]] = None,
    stream_key: str = "orka:memory",
    debug_keep_previous_outputs: bool = False,
    decay_config: Optional[Dict[str, Any]] = None,
    enable_hnsw: bool = True,
    vector_params: Optional[Dict[str, Any]] = None,
    **kwargs: Any,
) -> None:
    """Initialize the RedisStack memory logger."""
    # Handle legacy decay config
    effective_decay_config = memory_decay_config or decay_config
    
    super().__init__(
        stream_key,
        debug_keep_previous_outputs,
        effective_decay_config,
    )
    
    self.redis_url = redis_url
    self.index_name = index_name
    self.embedder = embedder
    self.enable_hnsw = enable_hnsw
    self.vector_params = vector_params or {}
    self.stream_key = stream_key
    self.debug_keep_previous_outputs = debug_keep_previous_outputs
    
    # Thread safety for parallel operations
    self._connection_lock = Lock()
    self._embedding_lock = Lock()
    self._local = threading.local()
    
    # Primary Redis connection
    self.redis_client = self._create_redis_connection()
    
    # Ensure the enhanced memory index exists
    self._ensure_index()
```

Implementation Strategy:

1. First, we'll update the template rendering system to properly expose variables at the root level.
2. Then, we'll modify the memory writer to correctly extract and store all memory objects.
3. Next, we'll update the loop node to properly handle past loops and their metadata.
4. Finally, we'll ensure the memory logger is properly configured for all components.



-----------------------------

Let me analyze the current code and the proposed changes to evaluate their effectiveness:
Ran tool
Let me check the current memory writer implementation:
Ran tool
After analyzing the current code and the proposed changes, here's my assessment of the implementation suggestions:

1. **Template Rendering Changes** (Efficiency: 80%, Success Chance: High)
```python
def _enhance_payload_for_templates(self, payload: Dict[str, Any]) -> Dict[str, Any]:
    enhanced_payload = payload.copy()
    # ... proposed implementation ...
```
**Analysis**:
- ✅ The proposed changes correctly expose variables at the root level
- ✅ Handles nested structures properly
- ✅ Maintains type safety
- ⚠️ Potential Issue: Need to ensure we don't override existing keys accidentally
- 🔧 Improvement Needed: Add error handling for malformed input structures

2. **Memory Storage Changes** (Efficiency: 70%, Success Chance: Medium)
```python
def _extract_memory_content(self, context: Dict[str, Any]) -> List[str]:
    memory_objects: List[str] = []
    # ... proposed implementation ...
```
**Analysis**:
- ✅ Correctly handles both validation guardians
- ✅ Supports both string and dictionary memory objects
- ⚠️ Potential Issue: Doesn't handle nested memory objects deeply enough
- 🔧 Improvement Needed: Add validation for memory object structure
- 🔧 Improvement Needed: Add support for memory object metadata

3. **Past Loops Changes** (Efficiency: 90%, Success Chance: High)
```python
async def _execute_internal_workflow(self, original_input: Any, previous_outputs: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    # ... proposed implementation ...
```
**Analysis**:
- ✅ Properly handles memory logger inheritance
- ✅ Correctly passes past loops metadata
- ✅ Maintains type safety with TypedDict
- ✅ Preserves orchestrator configuration
- ⚠️ Potential Issue: Temporary file handling could be improved

Recommended Modifications:

1. **Template Rendering**:
```python
def _enhance_payload_for_templates(self, payload: Dict[str, Any]) -> Dict[str, Any]:
    enhanced_payload = payload.copy()
    
    # Expose input properties safely
    if isinstance(enhanced_payload.get("input"), dict):
        input_data = enhanced_payload["input"]
        for key, value in input_data.items():
            if key not in enhanced_payload:
                enhanced_payload[key] = value
    
    # Expose previous outputs safely
    if "previous_outputs" in enhanced_payload:
        for agent_id, output in enhanced_payload["previous_outputs"].items():
            if isinstance(output, dict):
                # Expose direct fields
                for field in ["response", "formatted_prompt", "internal_reasoning"]:
                    key = f"{agent_id}_{field}"
                    if field in output and key not in enhanced_payload:
                        enhanced_payload[key] = output[field]
                
                # Expose nested result fields
                if "result" in output and isinstance(output["result"], dict):
                    for field, value in output["result"].items():
                        key = f"{agent_id}_{field}"
                        if key not in enhanced_payload:
                            enhanced_payload[key] = value
    
    return enhanced_payload
```

2. **Memory Storage**:
```python
def _extract_memory_content(self, context: Dict[str, Any]) -> List[str]:
    memory_objects: List[str] = []
    previous_outputs = context.get("previous_outputs", {})
    
    def extract_memory_objects(result: Any) -> None:
        if isinstance(result, dict):
            # Direct memory objects
            if "memory_objects" in result:
                for mem_obj in result["memory_objects"]:
                    if isinstance(mem_obj, str):
                        memory_objects.append(mem_obj)
                    elif isinstance(mem_obj, dict) and "content" in mem_obj:
                        memory_objects.append(mem_obj["content"])
            
            # Nested memory objects
            for value in result.values():
                if isinstance(value, dict):
                    extract_memory_objects(value)
    
    # Process both validation guardians
    for guardian_name in ["true_validation_guardian", "false_validation_guardian"]:
        guardian_output = previous_outputs.get(guardian_name, {})
        if isinstance(guardian_output, dict) and "result" in guardian_output:
            extract_memory_objects(guardian_output["result"])
    
    return memory_objects
```

3. **Past Loops**:
```python
async def _execute_internal_workflow(
    self,
    original_input: Any,
    previous_outputs: Dict[str, Any]
) -> Optional[Dict[str, Any]]:
    """Execute the internal workflow configuration."""
    # Create orchestrator configuration
    workflow_config = {
        "orchestrator": {
            "id": "internal-workflow",
            "strategy": "sequential",
            "memory": {
                "config": {
                    "redis_url": os.getenv("REDIS_URL", "redis://localhost:6380/0"),
                    "index_name": self.memory.index_name if self.memory else "orka_enhanced_memory",
                    "embedding_model": self.memory.embedding_model if self.memory else "text-embedding-ada-002",
                    "embedding_dim": self.memory.embedding_dim if self.memory else 1536,
                    "default_short_term_hours": self.memory.default_short_term_hours if self.memory else 0.1,
                    "default_long_term_hours": self.memory.default_long_term_hours if self.memory else 0.2,
                    "check_interval_minutes": self.memory.check_interval_minutes if self.memory else 30,
                }
            }
        },
        **self.internal_workflow
    }
    
    # Prepare input with past loops context
    current_loop_number = len(previous_outputs.get("past_loops", [])) + 1
    workflow_input = {
        "input": original_input,
        "previous_outputs": previous_outputs,
        "loop_number": current_loop_number,
        "past_loops_metadata": {
            "insights": self._extract_metadata_field("insights", previous_outputs.get("past_loops", [])),
            "improvements": self._extract_metadata_field("improvements", previous_outputs.get("past_loops", [])),
            "mistakes": self._extract_metadata_field("mistakes", previous_outputs.get("past_loops", [])),
        }
    }
    
    # Create and run orchestrator
    orchestrator = Orchestrator(workflow_config, memory_logger=self.memory)
    return await orchestrator.run(workflow_input)
```

Overall Assessment:
1. The changes are well-structured and maintain type safety
2. They address the core issues while preserving existing functionality
3. The implementation matches `orka_or`'s behavior
4. Success chance is high with the recommended modifications

Would you like me to proceed with implementing these modified versions?