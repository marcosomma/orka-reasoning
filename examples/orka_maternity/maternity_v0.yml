# Maternal-Instinct Safety Workflow — minimal, testable, and shippable in a day
# Updated to use only local LLM agents (gpt-oss:20b) for all reasoning and generation.

orchestrator:
  id: maternal_instinct_v0
  strategy: sequential
  queue: redis
  memory_preset: "episodic"  # Use episodic memory for interaction history
  agents:
    - fork_metrics
    - metrics_join
    - route_determiner
    - maternal_policy_router
    - final_summary


agents:
  - id: fork_metrics
    type: fork
    targets:
      - policies_rag
      - prior_incidents_reader
      - risk_predictor
      - vulnerability_analyzer
      - reversibility_estimator
      - consent_checker

  - id: policies_rag
    type: memory
    memory_preset: "semantic"  # Use semantic memory for knowledge/policies
    namespace: policy_notes
    config:
      operation: read
      limit: 3  # Reduced for efficiency
      similarity_threshold: 0.7  # Use preset default
    prompt: |
      Fetch policies, medical safety notes, and app rules relevant to: {{ get_input() }}
      Return a compact bullet list.

  - id: prior_incidents_reader
    type: memory
    memory_preset: "episodic"  # Use episodic memory for incident history
    namespace: safety_events
    config:
      operation: read
      limit: 5
      similarity_threshold: 0.6  # Use preset default
      enable_temporal_ranking: true
    prompt: |
      Retrieve prior harmful or near-harmful incidents similar to: {{ get_input() }}
      Include stored metrics and timestamps.

  - id: risk_predictor
    type: local_llm
    model: gpt-oss:20b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.3
    prompt: |
      Predict harm score H ∈ [0,1] for the request using context.
      Factors: physical harm, psychological distress, legal exposure, irreversibility.
      Return JSON: {"H": float, "rationale": "..."}.
      
      Request: {{ get_input() }}

  - id: vulnerability_analyzer
    type: local_llm
    model: gpt-oss:20b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.3
    prompt: |
      Infer vulnerability V ∈ [0,1] from the request and prior incidents.
      Consider minors, dependency, coercion, crisis cues.
      Return JSON: {"V": float, "signals": ["..."]}.
      
      Request: {{ get_input() }}
      Prior incidents: {{ safe_get_response('prior_incidents_reader', 'No prior incidents') }}

  - id: reversibility_estimator
    type: local_llm
    model: gpt-oss:20b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.3
    prompt: |
      Estimate reversibility R ∈ [0,1]; 1 = fully reversible.
      Return JSON: {"R": float, "notes": "..."}.
      
      Request: {{ get_input() }}

  - id: consent_checker
    type: local_llm
    model: gpt-oss:20b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.2
    prompt: |
      Is explicit informed consent present for the risky action?
      Return JSON: {"C": "verified" | "unclear" | "denied", "evidence": "..."}.
      
      Request: {{ get_input() }}

  - id: metrics_join
    type: join
    group: fork_metrics
    prompt: |
      Combine metrics from upstream. maternal_score = V*H.
      Apply memory escalation: effective_H = clamp(H + 0.15 * min(prior_severe_count, 3), 0, 1).
      Output JSON with all metrics.
      
      Risk: {{ safe_get_response('risk_predictor', '{}') }}
      Vulnerability: {{ safe_get_response('vulnerability_analyzer', '{}') }}
      Reversibility: {{ safe_get_response('reversibility_estimator', '{}') }}
      Consent: {{ safe_get_response('consent_checker', '{}') }}

  - id: route_determiner
    type: local_llm
    model: gpt-oss:20b
    prompt: |
      Analyze the metrics and determine the appropriate safety route.
      
      Metrics: {{ safe_get_response('metrics_join', '{}') }}
      
      Decision rules:
      - If H >= 0.25 OR R <= 0.4: output exactly "safeguard_gate"
      - If H >= 0.15 OR consent is unclear: output exactly "caution_path" 
      - Otherwise: output exactly "normal_path"
      
      Return ONLY one of these exact strings: "safeguard_gate", "caution_path", or "normal_path".

  - id: maternal_policy_router
    type: router
    params:
      decision_key: route_determiner
      routing_map:
        safeguard_gate: [responder_block, empathy_scorer, memory_writer, explainer]
        caution_path: [safe_alternative_builder, responder_caution, empathy_scorer, memory_writer, explainer]
        normal_path: [responder_normal, empathy_scorer, memory_writer, explainer]
    prompt: |
      Inputs: {{ safe_get_response('metrics_join', '{}') }}
      Route: {{ safe_get_response('route_determiner', 'normal_path') }}

  - id: safe_alternative_builder
    type: local_llm
    model: gpt-oss:20b
    prompt: |
      Suggest a safer, reversible alternative based on policies and the request.
      
      Request: {{ get_input() }}
      Policies: {{ safe_get_response('policies_rag', 'No specific policies found') }}

  - id: responder_block
    type: local_llm
    model: gpt-oss:20b
    prompt: |
      Block the risky request. Provide protective rationale, reassurance, and a safer alternative.
      
      Request: {{ get_input() }}
      Metrics: {{ safe_get_response('metrics_join', '{}') }}

  - id: responder_caution
    type: local_llm
    model: gpt-oss:20b
    prompt: |
      Decline risky portion and propose the safer alternative while maintaining rapport.
      
      Request: {{ get_input() }}
      Alternative: {{ safe_get_response('safe_alternative_builder', 'No specific alternative available') }}

  - id: responder_normal
    type: local_llm
    model: gpt-oss:20b
    prompt: |
      Proceed safely with clarity and boundary reminders.
      
      Request: {{ get_input() }}
      Metrics: {{ safe_get_response('metrics_join', '{}') }}

  - id: empathy_scorer
    type: local_llm
    model: gpt-oss:20b
    prompt: |
      Score comfort/clarity (K) of the final response in [0,1].
      
      Response: {{ safe_get_response('responder_block', '') }} {{ safe_get_response('responder_caution', '') }} {{ safe_get_response('responder_normal', '') }}

  - id: memory_writer
    type: memory
    namespace: safety_events
    config:
      operation: write
      vector: true
      vector_field: "content_vector"  # Ensure proper vector field for RedisStack
      # Enhanced vector configuration
      force_recreate_index: false
      vector_params:
        type: "FLOAT32"
        distance_metric: "COSINE"
        ef_construction: 200
        m: 16
      memory_type: short_term
      metadata:
        source: "maternal_instinct_v0"
    prompt: |
      Store event with metrics, route, and comfort score.
      
      Request: {{ get_input() }}
      Metrics: {{ safe_get_response('metrics_join', '{}') }}
      Route: {{ safe_get_response('route_determiner', 'unknown') }}
      Comfort: {{ safe_get_response('empathy_scorer', '{}') }}

  - id: explainer
    type: local_llm
    model: gpt-oss:20b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.7
    prompt: |
      Summarize rationale with metrics and chosen route.
      
      Request: {{ get_input() }}
      Metrics: {{ safe_get_response('metrics_join', '{}') }}
      Route: {{ safe_get_response('route_determiner', 'unknown') }}
      Response: {{ safe_get_response('responder_block', '') }} {{ safe_get_response('responder_caution', '') }} {{ safe_get_response('responder_normal', '') }}

  - id: final_summary
    type: local_llm
    model: gpt-oss:20b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.7
    prompt: |
      Provide a comprehensive final summary of the maternal safety analysis:
      
      Original Request: {{ get_input() }}
      
      Safety Analysis:
      {% set risk_result = safe_get_response('risk_predictor', '{}') %}
      {% if risk_result %}
      Risk Assessment: {{ risk_result }}
      {% endif %}
      
      {% set vulnerability_result = safe_get_response('vulnerability_analyzer', '{}') %}
      {% if vulnerability_result %}
      Vulnerability Analysis: {{ vulnerability_result }}
      {% endif %}
      
      {% set route_decision = safe_get_response('route_determiner', 'unknown') %}
      {% if route_decision %}
      Safety Route: {{ route_decision }}
      {% endif %}
      
      {% set explanation = safe_get_response('explainer', '') %}
      {% if explanation %}
      Detailed Explanation: {{ explanation }}
      {% endif %}
      
      Provide a clear, comprehensive final response that prioritizes user safety while being empathetic and helpful.
